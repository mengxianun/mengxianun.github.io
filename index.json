[{"authors":["admin"],"categories":null,"content":"Nelson Bighetti is a professor of artificial intelligence at the Stanford AI Lab. His research interests include distributed robotics, mobile computing and programmable matter. He leads the Robotic Neurobiology group, which develops self-reconfiguring robots, systems of self-organizing robots, and mobile sensor networks.\nLorem ipsum dolor sit amet, consectetur adipiscing elit. Sed neque elit, tristique placerat feugiat ac, facilisis vitae arcu. Proin eget egestas augue. Praesent ut sem nec arcu pellentesque aliquet. Duis dapibus diam vel metus tempus vulputate.\n","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"/author/nelson-bighetti/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/nelson-bighetti/","section":"authors","summary":"Nelson Bighetti is a professor of artificial intelligence at the Stanford AI Lab. His research interests include distributed robotics, mobile computing and programmable matter. He leads the Robotic Neurobiology group, which develops self-reconfiguring robots, systems of self-organizing robots, and mobile sensor networks.","tags":null,"title":"Nelson Bighetti","type":"authors"},{"authors":[],"categories":[],"content":"服务端配置 /etc/frp/frps.ini\n[common]\rbind_port = 7000\rtoken = ***\r 客户端配置 /etc/frp/frpc.ini\n[common]\rserver_addr = 服务端IP(公网)\rserver_port = 7000\rtoken = ***\r[gitlab]\rtype = tcp\rlocal_ip = 内网服务IP\rlocal_port = 内网服务端口\rremote_port = 公网服务端口\r 管理 重启 systemctl restart frpc.service\r ","date":1650765730,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1650765730,"objectID":"9fb835072f4de9e5cfb43d01fbbc5725","permalink":"/posts/frp/","publishdate":"2022-04-24T10:02:10+08:00","relpermalink":"/posts/frp/","section":"posts","summary":"服务端配置 /etc/frp/frps.ini [common] bind_port = 7000 token = *** 客户端配置 /etc/frp/frpc.ini [common] server_addr = 服务端IP(公网","tags":[],"title":"Frp","type":"posts"},{"authors":[],"categories":[],"content":"安装 docker docker run -d -p 27017:27017 --name metasearch-mongo -e MONGO_INITDB_DATABASE=db_name -e MONGO_INITDB_ROOT_USERNAME=username -e MONGO_INITDB_ROOT_PASSWORD=password -v /data/mongo:/data/db mongo\r ","date":1650511096,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1650511096,"objectID":"83d5f02ef187ef105c93ecefa15ccd96","permalink":"/posts/mongo/","publishdate":"2022-04-21T11:18:16+08:00","relpermalink":"/posts/mongo/","section":"posts","summary":"安装 docker docker run -d -p 27017:27017 --name metasearch-mongo -e MONGO_INITDB_DATABASE=db_name -e MONGO_INITDB_ROOT_USERNAME=username -e MONGO_INITDB_ROOT_PASSWORD=password -v /data/mongo:/data/db mongo","tags":[],"title":"Mongo","type":"posts"},{"authors":[],"categories":[],"content":"安装（Red Hat 7, CentOS 7） 导入签名密钥 ## primary RabbitMQ signing key\rrpm --import https://github.com/rabbitmq/signing-keys/releases/download/2.0/rabbitmq-release-signing-key.asc\r## modern Erlang repository\rrpm --import https://packagecloud.io/rabbitmq/erlang/gpgkey\r## RabbitMQ server repository\rrpm --import https://packagecloud.io/rabbitmq/rabbitmq-server/gpgkey\r 添加 Yum 存储库 # In /etc/yum.repos.d/rabbitmq.repo\r##\r## Zero dependency Erlang\r##\r[rabbitmq_erlang]\rname=rabbitmq_erlang\rbaseurl=https://packagecloud.io/rabbitmq/erlang/el/7/$basearch\rrepo_gpgcheck=1\rgpgcheck=1\renabled=1\r# PackageCloud's repository key and RabbitMQ package signing key\rgpgkey=https://packagecloud.io/rabbitmq/erlang/gpgkey\rhttps://github.com/rabbitmq/signing-keys/releases/download/2.0/rabbitmq-release-signing-key.asc\rsslverify=1\rsslcacert=/etc/pki/tls/certs/ca-bundle.crt\rmetadata_expire=300\r[rabbitmq_erlang-source]\rname=rabbitmq_erlang-source\rbaseurl=https://packagecloud.io/rabbitmq/erlang/el/7/SRPMS\rrepo_gpgcheck=1\rgpgcheck=0\renabled=1\rgpgkey=https://packagecloud.io/rabbitmq/erlang/gpgkey\rsslverify=1\rsslcacert=/etc/pki/tls/certs/ca-bundle.crt\rmetadata_expire=300\r##\r## RabbitMQ server\r##\r[rabbitmq_server]\rname=rabbitmq_server\rbaseurl=https://packagecloud.io/rabbitmq/rabbitmq-server/el/7/$basearch\rrepo_gpgcheck=1\rgpgcheck=1\renabled=1\r# PackageCloud's repository key and RabbitMQ package signing key\rgpgkey=https://packagecloud.io/rabbitmq/rabbitmq-server/gpgkey\rhttps://github.com/rabbitmq/signing-keys/releases/download/2.0/rabbitmq-release-signing-key.asc\rsslverify=1\rsslcacert=/etc/pki/tls/certs/ca-bundle.crt\rmetadata_expire=300\r[rabbitmq_server-source]\rname=rabbitmq_server-source\rbaseurl=https://packagecloud.io/rabbitmq/rabbitmq-server/el/7/SRPMS\rrepo_gpgcheck=1\rgpgcheck=0\renabled=1\rgpgkey=https://packagecloud.io/rabbitmq/rabbitmq-server/gpgkey\rsslverify=1\rsslcacert=/etc/pki/tls/certs/ca-bundle.crt\rmetadata_expire=300\r 安装 ## Update Yum package metadata\ryum update -y\r## install the packages\r## install these dependencies from standard OS repositories\ryum install socat logrotate -y\ryum install erlang rabbitmq-server -y\r 启动服务 ## 所有节点\rsystemctl enable rabbitmq-server.service\rsystemctl start rabbitmq-server.service\r## 主节点\r拷贝 /var/lib/rabbitmq/.erlang.cookie 到副本节点\r## 副本节点\rrabbitmqctl stop_app # =\u0026gt; 停止节点 rabbit@rabbit2 ...完成。\rrabbitmqctl reset # =\u0026gt; 重置节点 rabbit@rabbit2 ... rabbitmqctl join_cluster rabbit@rabbit1 # =\u0026gt; 使用 [rabbit@rabbit1] 聚类节点 rabbit@rabbit2 ...完成。\rrabbitmqctl start_app # =\u0026gt; 开始节点 rabbit@rabbit2 ...完成。\r 操作 创建vhost rabbitmqctl add_vhost qa1\r 创建用户 rabbitmqctl add_user \u0026quot;username\u0026quot; \u0026quot;password\u0026quot;\r 用户授权 rabbitmqctl set_permissions -p \u0026quot;qa1\u0026quot; \u0026quot;iie\u0026quot; \u0026quot;.*\u0026quot; \u0026quot;.*\u0026quot; \u0026quot;.*\u0026quot;\r 启用UI ## Enable\rrabbitmq-plugins enable rabbitmq_management\r## tag the user with \u0026quot;administrator\u0026quot; for full management UI and HTTP API access\rrabbitmqctl set_user_tags full_access administrator\r## Access http://{node-hostname}:15672/\r ","date":1648521407,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1648521407,"objectID":"b71d245f3ee425dd3c24d4d409b23f6d","permalink":"/posts/rabbit/","publishdate":"2022-03-29T10:36:47+08:00","relpermalink":"/posts/rabbit/","section":"posts","summary":"安装（Red Hat 7, CentOS 7） 导入签名密钥 ## primary RabbitMQ signing key rpm --import https://github.com/rabbitmq/signing-keys/releases/download/2.0/rabbitmq-release-signing-key.asc ## modern Erlang repository rpm","tags":[],"title":"Rabbit","type":"posts"},{"authors":[],"categories":[],"content":"Kafka\n维护 查看所有consumer kafka-consumer-groups.sh --list --bootstrap-server localhost:9092\r 查看offset kafka-consumer-offset-checker.sh --group SampleConsumer --topic topicName --zookeeper localhost:2181\r 重置offset kafka-consumer-groups.sh --bootstrap-server localhost:9092 --group SampleConsumer --reset-offsets --to-earliest --topic topicName --execute\r ","date":1612848776,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1612848776,"objectID":"79fff22a3583c060072824433949d050","permalink":"/posts/kafka/","publishdate":"2021-02-09T13:32:56+08:00","relpermalink":"/posts/kafka/","section":"posts","summary":"Kafka 维护 查看所有consumer kafka-consumer-groups.sh --list --bootstrap-server localhost:9092 查看offset kafka-consumer-offset-checker.sh --group SampleConsumer","tags":[],"title":"Kafka","type":"posts"},{"authors":[],"categories":[],"content":"IDEA\n配置 \u0026hellip;\n问题 控制台乱码 Help-\u0026gt;Edit Custom VM Options\u0026hellip;，添加-Dfile.encoding=UTF-8\n","date":1612775734,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1612775734,"objectID":"134847fa46d7e0668e4ab94e460c6f55","permalink":"/posts/idea/","publishdate":"2021-02-08T17:15:34+08:00","relpermalink":"/posts/idea/","section":"posts","summary":"IDEA 配置 \u0026hellip; 问题 控制台乱码 Help-\u0026gt;Edit Custom VM Options\u0026hellip;，","tags":[],"title":"IDEA","type":"posts"},{"authors":[],"categories":[],"content":"ELK\nDocker Compose方式部署 采集docker容器日志\n目录结构 . ├── docker-compose.yml ├── filebeat.yml └── pipeline └── logstash.conf 1 directory, 3 files  docker-compose.yml version: '3.3' services: elasticsearch: hostname: elasticsearch image: docker.elastic.co/elasticsearch/elasticsearch:$ELASTIC_VERSION environment: - TZ=Asia/Shanghai - bootstrap.memory_lock=true - \u0026quot;ES_JAVA_OPTS=-Xms512m -Xmx512m\u0026quot; - discovery.type=single-node volumes: - esdata1:/usr/share/elasticsearch/data ports: - 9200:9200 networks: - stack kibana: hostname: kibana image: docker.elastic.co/kibana/kibana:$ELASTIC_VERSION environment: - TZ=Asia/Shanghai depends_on: - elasticsearch ports: - 5601:5601 networks: - stack logstash: hostname: logstash image: docker.elastic.co/logstash/logstash:$ELASTIC_VERSION environment: - TZ=Asia/Shanghai volumes: - ./pipeline/:/usr/share/logstash/pipeline/ ports: - 5044:5044 depends_on: - elasticsearch networks: - stack filebeat: hostname: filebeat image: docker.elastic.co/beats/filebeat:${ELASTIC_VERSION} environment: - TZ=Asia/Shanghai user: root volumes: - ./filebeat.yml:/usr/share/filebeat/filebeat.yml:ro - /var/lib/docker/containers:/var/lib/docker/containers:ro - /var/run/docker.sock:/var/run/docker.sock:ro command: filebeat -e depends_on: - elasticsearch - kibana restart: on-failure networks: - stack volumes: esdata1: driver: local networks: stack:  filebeat.yml filebeat.autodiscover: providers: - type: docker templates: - condition: contains: docker.container.labels.com.docker.compose.project.working_dir: metasearch config: - type: container paths: - /var/lib/docker/containers/${data.docker.container.id}/*.log multiline.pattern: '^([0-9]{4}-[0-9]{2}-[0-9]{2})' multiline.negate: true multiline.match: after output.logstash: hosts: [\u0026quot;logstash:5044\u0026quot;]  logstash.conf input { beats { port =\u0026gt; 5044 } } filter { grok { match =\u0026gt; { \u0026quot;message\u0026quot; =\u0026gt; [ \u0026quot;(?m)%{TIMESTAMP_ISO8601:timestamp}%{SPACE}%{LOGLEVEL:level} %{NUMBER} --- \\[\\s*%{DATA:thread}\\] [A-Za-z0-9.]*\\.(?\u0026lt;class\u0026gt;[A-Za-z0-9#_$]+)\\s*:\\s+%{GREEDYDATA:logmessage}\u0026quot;, \u0026quot;(?m)%{TIMESTAMP_ISO8601:timestamp}%{SPACE}%{LOGLEVEL:level} \\[%{DATA:thread}\\][A-Za-z0-9.]*\\.(?\u0026lt;class\u0026gt;[A-Za-z0-9#_$]+)\\.(?\u0026lt;method\u0026gt;[A-Za-z0-9#_$\u0026lt;\u0026gt;]+):%{NUMBER:line} -%{GREEDYDATA:logmessage}\u0026quot; ] } } date { match =\u0026gt; [\u0026quot;timestamp\u0026quot;, \u0026quot;ISO8601\u0026quot;, \u0026quot;yyyy-MM-dd HH:mm:ss.SSS\u0026quot;] } mutate { remove_field =\u0026gt; [\u0026quot;docker\u0026quot;, \u0026quot;container\u0026quot;, \u0026quot;agent\u0026quot;, \u0026quot;ecs\u0026quot;, \u0026quot;stream\u0026quot;] add_field =\u0026gt; { \u0026quot;service\u0026quot; =\u0026gt; \u0026quot;%{[docker][container][labels][com_docker_compose_service]}\u0026quot; } } } output { elasticsearch { hosts =\u0026gt; [\u0026quot;http://elasticsearch:9200\u0026quot;] index =\u0026gt; \u0026quot;metasearch-log-%{+YYYY.MM.dd}\u0026quot; } }  ","date":1610940785,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1610940785,"objectID":"bdcf3e39a36dc80f900bc4ec044b681b","permalink":"/posts/elk/","publishdate":"2021-01-18T11:33:05+08:00","relpermalink":"/posts/elk/","section":"posts","summary":"ELK Docker Compose方式部署 采集docker容器日志 目录结构 .","tags":[],"title":"ELK","type":"posts"},{"authors":[],"categories":[],"content":"MinIO\n安装 wget https://dl.min.io/server/minio/release/linux-amd64/minio -O /usr/local/bin/minio\rchmod +x /usr/local/bin/minio\r 环境变量 export MINIO_ACCESS_KEY=\u0026lt;ACCESS_KEY\u0026gt;\rexport MINIO_SECRET_KEY=\u0026lt;SECRET_KEY\u0026gt;\r 单节点 minio server /data\r 分布式 单节点多磁盘 minio server /data{1...4}\ror\rminio server /data1 /data2 /data3 /data4\r# 后台运行\rsetsid minio server /home/minio/data \u0026gt;/home/minio/data/minio.log 2\u0026gt;\u0026amp;1 \u0026lt; /dev/null \u0026amp;\r 多节点多磁盘 minio server http://host{1...n}/export{1...m}\ror\rminio server http://host1/export1 http://host1/export2 http://host2/export1 http://host2/export2 ...\r 注：\n 所有运行分布式 MinIO 的节点需要具有相同的访问密钥和秘密密钥才能连接。为了实现这一点，建议在执行 MINIO 服务器命令之前，将所有节点上的访问密钥和秘密密钥作为环境变量 MINIO _ access _ key 和 MINIO _ secret _ key 导出。 分布式 Minio 使用的磁盘里必须是干净的，里面没有数据。 MinIO 创建每组4到16个驱动器的擦除编码集。您提供的驱动器总数必须是这些数字之一的倍数。 _ MinIO 选择最大的 EC 设置大小，将其划分为驱动器的总数或给定的节点的总数——确保保持均匀分布，即每个节点参与每个设置的驱动器数相等。 建议所有运行分布式 MinIO 设置的节点都是同构的，即相同的操作系统、相同数量的磁盘和相同的网络互连。 运行分布式 MinIO 实例的服务器时间差不应超过15分钟。  服务 单节点   创建用户\nuseradd minio\rpasswd minio\r   创建默认配置文件\n$ cat \u0026lt;\u0026lt;EOT \u0026gt;\u0026gt; /etc/default/minio\r# Volume to be used for MinIO server.\rMINIO_VOLUMES=\u0026quot;/tmp/minio/\u0026quot;\r# Use if you want to run MinIO on a custom port.\rMINIO_OPTS=\u0026quot;--address :9199\u0026quot;\r# Access Key of the server.\rMINIO_ACCESS_KEY=Server-Access-Key\r# Secret key of the server.\rMINIO_SECRET_KEY=Server-Secret-Key\rEOT\r   下载 service 文件 https://raw.githubusercontent.com/minio/minio-service/master/linux-systemd/minio.service 到 /etc/systemd/system/，并替换里面的用户和组\n[Unit]\rDescription=MinIO\rDocumentation=https://docs.min.io\rWants=network-online.target\rAfter=network-online.target\rAssertFileIsExecutable=/usr/local/bin/minio\r[Service]\rWorkingDirectory=/usr/local/\rUser=minio-user\rGroup=minio-user\rEnvironmentFile=/etc/default/minio\rExecStartPre=/bin/bash -c \u0026quot;if [ -z \\\u0026quot;${MINIO_VOLUMES}\\\u0026quot; ]; then echo \\\u0026quot;Variable MINIO_VOLUMES not set in /etc/default/minio\\\u0026quot;; exit 1; fi\u0026quot;\rExecStart=/usr/local/bin/minio server $MINIO_OPTS $MINIO_VOLUMES\r# Let systemd restart this service always\rRestart=always\r# Specifies the maximum file descriptor number that can be opened by this process\rLimitNOFILE=65536\r# Disable timeout logic and wait until process is stopped\rTimeoutStopSec=infinity\rSendSIGKILL=no\r[Install]\rWantedBy=multi-user.target\r# Built for ${project.name}-${project.version} (${project.name})\r   启动\nsystemctl daemon-reload\rsystemctl start minio\rsystemctl enable minio\r   分布式   创建用户\nuseradd minio\rpasswd minio\r   创建存储目录\nmkdir /export\rsudo chown -R minio /export \u0026amp;\u0026amp; sudo chmod u+rxw /export\r   创建默认配置文件\n$ cat \u0026lt;\u0026lt;EOT \u0026gt;\u0026gt; /etc/default/minio\r# Remote volumes to be used for MinIO server.\rMINIO_VOLUMES=http://node{1...6}/export{1...32}\r# Use if you want to run MinIO on a custom port.\rMINIO_OPTS=\u0026quot;--address :9199\u0026quot;\r# Access Key of the server.\rMINIO_ACCESS_KEY=Server-Access-Key\r# Secret key of the server.\rMINIO_SECRET_KEY=Server-Secret-Key\rEOT\r   下载 service 文件 https://raw.githubusercontent.com/minio/minio-service/master/linux-systemd/distributed/minio.service 到 /etc/systemd/system/，并替换里面的用户和组\n[Unit]\rDescription=MinIO\rDocumentation=https://docs.min.io\rWants=network-online.target\rAfter=network-online.target\rAssertFileIsExecutable=/usr/local/bin/minio\r[Service]\rWorkingDirectory=/usr/local\rUser=minio-user\rGroup=minio-user\rEnvironmentFile=-/etc/default/minio\rExecStartPre=/bin/bash -c \u0026quot;if [ -z \\\u0026quot;${MINIO_VOLUMES}\\\u0026quot; ]; then echo \\\u0026quot;Variable MINIO_VOLUMES not set in /etc/default/minio\\\u0026quot;; exit 1; fi\u0026quot;\rExecStartPre=/bin/bash -c \u0026quot;if [ -z \\\u0026quot;${MINIO_ACCESS_KEY}\\\u0026quot; ]; then echo \\\u0026quot;Variable MINIO_ACCESS_KEY not set in /etc/default/minio\\\u0026quot;; exit 1; fi\u0026quot;\rExecStartPre=/bin/bash -c \u0026quot;if [ -z \\\u0026quot;${MINIO_SECRET_KEY}\\\u0026quot; ]; then echo \\\u0026quot;Variable MINIO_SECRET_KEY not set in /etc/default/minio\\\u0026quot;; exit 1; fi\u0026quot;\rExecStart=/usr/local/bin/minio server $MINIO_OPTS $MINIO_VOLUMES\r# Let systemd restart this service always\rRestart=always\r# Specifies the maximum file descriptor number that can be opened by this process\rLimitNOFILE=65536\r# Disable timeout logic and wait until process is stopped\rTimeoutStopSec=infinity\rSendSIGKILL=no\r[Install]\rWantedBy=multi-user.target\r# Built for ${project.name}-${project.version} (${project.name})\r   启动\nsystemctl daemon-reload\rsystemctl start minio\rsystemctl enable minio\r   客户端 安装 wget https://dl.min.io/client/mc/release/linux-amd64/mc -O /usr/local/bin/mc\rchmod +x /usr/local/bin/mc\r 常用命令    命令 说明     ls list buckets and objects            挂载minio为本地路径   安装goofys\nyum install -y fuse\rwget https://github.com/kahing/goofys/releases/latest/download/goofys -P /usr/local/bin/\rchmod +x /usr/local/bin/goofys\r   配置认证信息\n$ mkdir ~/.aws\r$ cat \u0026lt;\u0026lt;EOT \u0026gt; ~/.aws/credentials\r[default]\raws_access_key_id=Server-Access-Key\raws_secret_access_key=Server-Secret-Key\rEOT\r   挂载\ngoofys \u0026lt;bucket\u0026gt; \u0026lt;mountpoint\u0026gt;\rgoofys \u0026lt;bucket:prefix\u0026gt; \u0026lt;mountpoint\u0026gt; # if you only want to mount objects under a prefix\r# 示例\rgoofys --endpoint http://10.111.54.230:9000 test /minio-local-data\r   开机自动挂载\n# 修改 /etc/fstab, 添加如下内容\rgoofys#bucket /mnt/mountpoint fuse _netdev,allow_other,--file-mode=0666,--dir-mode=0777 0 0\r   卸载 fusermount -u /path/to/mountpoint\r 定时删除文件 ## 每天2点，删除30天前的文件\r0 2 * * * /root/minio-binaries/mc rm --recursive --dangerous --force --older-than 30d local\r ","date":1601003338,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1601003338,"objectID":"14dcebc68672fe8e9532854e9c2092b0","permalink":"/posts/minio/","publishdate":"2020-09-25T11:08:58+08:00","relpermalink":"/posts/minio/","section":"posts","summary":"MinIO 安装 wget https://dl.min.io/server/minio/release/linux-amd64/minio -O /usr/local/bin/minio chmod +x /usr/local/bin/minio 环境变量 export MINIO_ACCESS_KEY=\u0026lt;ACCESS_KEY\u0026gt; export MINIO_SECRET_KEY=\u0026lt;SECRET_KEY\u0026gt; 单节点 minio server /data 分布式 单节点","tags":[],"title":"MinIO安装","type":"posts"},{"authors":[],"categories":[],"content":"Linux - CentOS\n查看系统版本 cat /etc/redhat-release\r hostname   设置 hostname\nsudo hostnamectl set-hostname newname\r   修改 /etc/hosts\ncat \u0026gt;\u0026gt; /etc/hosts \u0026lt;\u0026lt; EOF\r192.168.1.100 domain1\r192.168.1.101 domain2\rEOF\r   重启networking\nsudo systemctl restart network\r   跟踪脚本 bash -x *.sh\r 挂载光盘为本地源   挂载光盘\nmount /dev/cdrom /mnt/cdrom\r   配置本地源文件CentOS-Media.repo\nbaseurl修改第二个路径为上步的挂载点(/mnt/cdrom)\renabled=1\r   禁用默认的yum网络源\n将yum 网络源配置文件改名为CentOS-Base.repo.bak,否则会先在网络源中寻找适合的包,改名之后直接从本地源读取.\r   硬盘挂载及扩容 新硬盘挂载 参考：http://www.linuxidc.com/Linux/2012-07/65294.htm\n  查看硬盘分区情况\nsudo fdisk -l\r   新硬盘分区\nsudo fdisk /dev/sda\rCommand (m for help)\rn：增加新分区\re：扩展分区\rPartition number(1-4)：默认1,只分一个区\rcylinder：默认\r分区大小：默认\rp：显示分区表\rw：保存分区表\r   查看分区\n  格式化\nsudo mkfs -t ext4 /dev/sda\r   挂载目录\nsudo mount -t ext4 /dev/sda /mnt/sda\r   查看\ndf -h\r   开机自动挂载(编辑/etc/fstab)\n在最后一行加入\r/dev/sda1 /mnt/sda1 ext4 defaults 1 2\r或者\rUUID=37eaa526-5d96-4237-8468-603df5216ce9 /mnt/sda ext4 defaults 0 3\r**UUID可以通过使用blkid命令来查看(sudo blkid /dev/sda)\r**０表示不备份；１表示要将整个\u0026lt;file system\u0026gt;中的内容备份。此处建议设置为０。\r**０表示不检查；挂载点为分区／（根分区）必须设置为１，其他的挂载点不能设置为１\r分区 挂载目录 文件格式\r   新硬盘扩容根节点   查看硬盘分区情况\nsudo fdisk -l\r   创建物理卷\npvcreate /dev/sda\r   查看创建好的物理卷\npvdisplay /dev/sda\r   卷组扩容\nvgextend gafis70-vg /dev/sda\r   查看扩容之后的卷组信息\nvgdisplay\r   逻辑卷扩容\n# 指定大小\rlvextend -l +8G /dev/gafis-vg/root\r# 全部\rlvextend -l +100%free /dev/gafis-vg/root\r   查看扩容之后的逻辑卷\nlvdisplay /dev/gafis-vg/root\r   文件系统扩容\nresize2fs /dev/mapper/gafis--vg-root\r   查看\ndf -h\r   ssh 免密   生成密钥\nssh-keygen -t rsa -P '' -f ~/.ssh/id_rsa\r   将公钥追加到认证文件中(本机免密)\ncat ~/.ssh/id_rsa.pub \u0026gt;\u0026gt; ~/.ssh/authorized_keys\r   将公钥追加到其他机器\nssh-copy-id root@remote_ip\r   免密2   生成密钥\nssh-keygen -t rsa -C \u0026quot;pass\u0026quot;\r文件名: 回车\r密码: pass\r   将公钥追加到其他机器\nssh-copy-id root@remote_ip\r   ssh-agent\nssh-agent bash\rssh-add .ssh/id_rsa\r输入密码\r   防火墙 \u0026ndash; firewalld默认开放ssh服务, 默认规则都添加到zone=public 参数 \u0026ndash;zone #作用域 \u0026ndash;add-port=80/tcp #添加端口，格式为：端口/通讯协议 \u0026ndash;permanent #永久生效，没有此参数重启后失效\n  开放http(80)服务\nfirewall-cmd --add-service=http\r   永久开放http服务\nfirewall-cmd --add-service=http --permanent\r   永久关闭\nfirewall-cmd --remove-service=http --permanent\r   重启防火墙生效\nfirewall-cmd --reload\r   查看\n# 所有\rfirewall-cmd --list-all\r# 端口\rfirewall-cmd --list-ports\r# 服务\rfirewall-cmd --list-services\r   查询服务启用状态\nfirewall-cmd --query-service http\r   永久开放端口\nfirewall-cmd --add-port=3128/tcp --permanent\r   ulimit（最大进程数和最大文件打开数)   查看\n# 最大文件打开数\rulimit -n\r# 最大进程数\rulimit -u\r   临时修改（重启后失效）\nulimit -n 204800\rulimit -u 204800\r   永久修改，修改 /etc/security/limits.conf 文件，在文件末尾添加\n* soft nofile 204800\r* hard nofile 204800\r* soft nproc 204800\r* hard nproc 204800\r* 代表针对所有用户 noproc 是代表最大进程数 nofile 是代表最大文件打开数\r   简化\necho \u0026quot;* soft nofile 65535\u0026quot; \u0026gt;\u0026gt; /etc/security/limits.conf\recho \u0026quot;* hard nofile 65535\u0026quot; \u0026gt;\u0026gt; /etc/security/limits.conf\recho \u0026quot;* soft nproc 65535\u0026quot; \u0026gt;\u0026gt; /etc/security/limits.conf\recho \u0026quot;* hard nproc 65535\u0026quot; \u0026gt;\u0026gt; /etc/security/limits.conf\r   vm.max_map_count   查看\nsysctl -a | grep vm.max_map_count\r   临时修改（重启后失效）\nsysctl -w vm.max_map_count=262144\r   永久修改，修改 /etc/sysctl.conf 文件，在文件末尾添加\nvm.max_map_count=262144\r   永久生效\nsysctl -p\r   后台守护进程 setsid myscript.sh \u0026gt;/dev/null 2\u0026gt;\u0026amp;1 \u0026amp;\r# 输出到日志文件\rsetsid myscript.sh \u0026gt;/path/to/logfile 2\u0026gt;\u0026amp;1 \u0026amp;\r swap 添加swap分区 # 添加8G的swap分区\rdd if=/dev/zero of=/home/swap bs=1024 count=8388608\rmkswap /home/swap\rswapon /home/swap\r 禁用swap sudo swapoff -a\rsudo sed -i '/ swap / s/^\\(.*\\)$/#\\1/g' /etc/fstab\r 禁用SELinux   查看状态\nsestatus\r   临时关闭\nsudo setenforce 0\r   永久关闭\nsed -i 's/enforcing/disabled/g' /etc/selinux/config /etc/selinux/config\r   监控文件夹 watch -n 0.1 ls \u0026lt;your_folder\u0026gt;\r 查看CPU个数 ## 方法1\rgrep -c ^processor /proc/cpuinfo\r## 方法2\rnproc\r## 方法3\rgetconf _NPROCESSORS_ONLN\r## 方法4\rlscpu | grep 'CPU(s):' | head -1 | awk '{print $2}'\r 代理上网 ## /etc/profile\rexport http_proxy=\u0026quot;http://proxy_username:proxy_password@proxy_ip:proxy_port\u0026quot;\r \u0026laquo;\u0026laquo;\u0026laquo;\u0026lt; HEAD\n目录结构查看 ## 目录\rls -R | grep \u0026quot;:$\u0026quot; | sed -e 's/:$//' -e 's/[^-][^\\/]*\\//--/g' -e 's/^/ /' -e 's/-/|/'\r## 文件\rfind . | sed -e \u0026quot;s/[^-][^\\/]*\\// |/g\u0026quot; -e \u0026quot;s/|\\([^ ]\\)/|-\\1/\u0026quot;\r=======\r#### 查看PID进程详情\r  cat /proc/PID/cmdline | tr \u0026lsquo;\\000\u0026rsquo; ' ' cat /proc/PID/cmdline | xargs -0 echo         a7ccb4a03d0b7eceeed9fc13f3c0ac57061f88ad\n       \r ","date":1600848538,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1600848538,"objectID":"b467b10587fb1ade16a24dde04d82025","permalink":"/posts/linux/","publishdate":"2020-09-23T16:08:58+08:00","relpermalink":"/posts/linux/","section":"posts","summary":"Linux - CentOS 查看系统版本 cat /etc/redhat-release hostname 设置 hostname sudo hostnamectl set-hostname newname 修改 /etc/hosts cat \u0026gt;\u0026gt; /etc/hosts \u0026lt;\u0026lt; EOF 192.168.1.100 domain1 192.168.1.101","tags":[],"title":"Linux","type":"posts"},{"authors":[],"categories":[],"content":"Redis Cluster 注：建议6个节点。在没有6个节点的情况下，每个节点可以创建2个实例\n  下载安装包\nwget http://download.redis.io/releases/redis-5.0.5.tar.gz\r   解压\ntar zxf redis-5.0.5.tar.gz -C /opt/\r   安装\nyum -y install gcc-c++\rcd /opt/redis-5.0.5/ \u0026amp;\u0026amp; make MALLOC=libc\r./utils/install_server.sh\r# 1 Please select the redis port for this instance: [6379] -- # 2 Please select the redis config file name [/etc/redis/6379.conf] --\r# 3 Please select the redis log file name [/var/log/redis_6379.log] --\r# 4 Please select the data directory for this instance [/var/lib/redis/6379] --\r# 5 Please select the redis executable path [] -- /opt/redis-5.0.5/src/redis-server\r   修改配置 /etc/redis/6379.conf\n# 绑定地址\rbind 0.0.0.0\r# 端口\rport 6379\r# 安全密码\rrequirepass foobared\r## =========集群相关=========\rcluster-enabled yes\rcluster-config-file nodes-6379.conf\rcluster-node-timeout 15000\rappendonly yes\r   启动\nsystemctl restart redis_6379\r   创建集群\n# 3个主节点, 3个副本节点\rredis-cli --cluster create 192.168.201.101:6379 192.168.201.101:6380 192.168.201.102:6379 192.168.201.102:6380 192.168.201.103:6379 192.168.201.103:6380 -a foobared --cluster-replicas 1\r   创建别名\n# 修改 ~/.bashrc，添加如下内容\ralias redis-cli=/opt/redis-5.0.5/src/redis-cli\ralias redis-server=/opt/redis-5.0.5/src/redis-server\r# 生效\rsource ~/.bashrc\r   ","date":1598951471,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1598951471,"objectID":"59ba13b2e9b05c34597cdb15943d1c8c","permalink":"/posts/redis/","publishdate":"2020-09-01T17:11:11+08:00","relpermalink":"/posts/redis/","section":"posts","summary":"Redis Cluster 注：建议6个节点。在没有6个节点的情况下，每个节点可以创","tags":[],"title":"Redis","type":"posts"},{"authors":[],"categories":[],"content":"goofys 挂载 minio 为本地路径\ngoofys   安装\nwget https://github.com/kahing/goofys/releases/latest/download/goofys -P /usr/local/bin/\rchmod +x /usr/local/bin/goofys\r   配置认证信息\n$ cat ~/.aws/credentials\r[default]\raws_access_key_id = AKID1234567890\raws_secret_access_key = MY-SECRET-KEY\r   挂载minio   挂载\ngoofys \u0026lt;bucket\u0026gt; \u0026lt;mountpoint\u0026gt;\rgoofys \u0026lt;bucket:prefix\u0026gt; \u0026lt;mountpoint\u0026gt; # if you only want to mount objects under a prefix\r# 示例\rgoofys --endpoint http://10.111.54.230:9000 test /minio-local-data\r   开机自动挂载\n# 修改 /etc/fstab, 添加如下内容\rgoofys#bucket /mnt/mountpoint fuse _netdev,allow_other,--file-mode=0666,--dir-mode=0777 0 0\r   ","date":1591943896,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1591943896,"objectID":"3f165c3523bbdd40b86b58fac7474814","permalink":"/posts/minio/goofys%E6%8C%82%E8%BD%BDminio%E4%B8%BA%E6%9C%AC%E5%9C%B0%E8%B7%AF%E5%BE%84/","publishdate":"2020-06-12T14:38:16+08:00","relpermalink":"/posts/minio/goofys%E6%8C%82%E8%BD%BDminio%E4%B8%BA%E6%9C%AC%E5%9C%B0%E8%B7%AF%E5%BE%84/","section":"posts","summary":"goofys 挂载 minio 为本地路径 goofys 安装 wget https://github.com/kahing/goofys/releases/latest/download/goofys -P /usr/local/bin/ chmod +x /usr/local/bin/goofys 配置认证信息 $ cat ~/.aws/credentials [default] aws_access_key_id","tags":[],"title":"Goofys挂载minio为本地路径","type":"posts"},{"authors":[],"categories":[],"content":"通过 helm 在 kubernetes 上安装 minio\n前提  3台服务器，CentOS7 系统 已安装Kubernetes环境  安装helm3   下载安装包 https://github.com/helm/helm/releases，选择 Linux amd64 版本\n  安装\ntar -zxvf helm-v3.0.0-linux-amd64.tar.gz\rmv linux-amd64/helm /usr/local/bin/helm\r   添加仓库\nhelm repo add stable https://kubernetes-charts.storage.googleapis.com/\r   安装kubeapps   安装\nhelm repo add bitnami https://charts.bitnami.com/bitnami\rkubectl create namespace kubeapps\rhelm install kubeapps --namespace kubeapps bitnami/kubeapps --set useHelm3=true\r   创建 Kubernetes API token\nkubectl create serviceaccount kubeapps-operator\rkubectl create clusterrolebinding kubeapps-operator --clusterrole=cluster-admin --serviceaccount=default:kubeapps-operator\r   查看 token\nkubectl get secret $(kubectl get serviceaccount kubeapps-operator -o jsonpath='{range .secrets[*]}{.name}{\u0026quot;\\n\u0026quot;}{end}' | grep kubeapps-operator-token) -o jsonpath='{.data.token}' -o go-template='{{.data.token | base64decode}}' \u0026amp;\u0026amp; echo\r   开启 Dashboard\nkubectl port-forward -n kubeapps --address 0.0.0.0 svc/kubeapps 8080:80\r   创建minio PV   创建 StorageClass\napiVersion: storage.k8s.io/v1\rkind: StorageClass\rmetadata:\rname: local-storage-minio\rprovisioner: kubernetes.io/no-provisioner\rvolumeBindingMode: WaitForFirstConsumer\r   创建 minio 需要的 PV\n这里在 k8s的2个 worker 节点上创建了4个pv，因为minio分布式最少需要4个节点，所里这里在2个worker节点上分别创建了2个存储目录。\napiVersion: v1\rkind: PersistentVolume\rmetadata:\rname: local-storage-minio-0\rspec:\rcapacity:\rstorage: 2Gi\raccessModes:\r- ReadWriteOnce\rpersistentVolumeReclaimPolicy: Retain\rstorageClassName: local-storage-minio\rlocal:\rpath: /localdata/minio/data-0\rnodeAffinity:\rrequired:\rnodeSelectorTerms:\r- matchExpressions:\r- key: kubernetes.io/hostname\roperator: In\rvalues:\r- node2\r---\rapiVersion: v1\rkind: PersistentVolume\rmetadata:\rname: local-storage-minio-1\rspec:\rcapacity:\rstorage: 2Gi\raccessModes:\r- ReadWriteOnce\rpersistentVolumeReclaimPolicy: Retain\rstorageClassName: local-storage-minio\rlocal:\rpath: /localdata/minio/data-1\rnodeAffinity:\rrequired:\rnodeSelectorTerms:\r- matchExpressions:\r- key: kubernetes.io/hostname\roperator: In\rvalues:\r- node2\r---\rapiVersion: v1\rkind: PersistentVolume\rmetadata:\rname: local-storage-minio-2\rspec:\rcapacity:\rstorage: 2Gi\raccessModes:\r- ReadWriteOnce\rpersistentVolumeReclaimPolicy: Retain\rstorageClassName: local-storage-minio\rlocal:\rpath: /localdata/minio/data-2\rnodeAffinity:\rrequired:\rnodeSelectorTerms:\r- matchExpressions:\r- key: kubernetes.io/hostname\roperator: In\rvalues:\r- node3\r---\rapiVersion: v1\rkind: PersistentVolume\rmetadata:\rname: local-storage-minio-3\rspec:\rcapacity:\rstorage: 2Gi\raccessModes:\r- ReadWriteOnce\rpersistentVolumeReclaimPolicy: Retain\rstorageClassName: local-storage-minio\rlocal:\rpath: /localdata/minio/data-3\rnodeAffinity:\rrequired:\rnodeSelectorTerms:\r- matchExpressions:\r- key: kubernetes.io/hostname\roperator: In\rvalues:\r- node3\r   手动创建PV目录\n# node2\rmkdir -p /localdata/minio/data-0\rmkdir -p /localdata/minio/data-1\r# node3\rmkdir -p /localdata/minio/data-2\rmkdir -p /localdata/minio/data-3\r   kubeapps 部署 minio   搜索 Chart，选择 bitnami 仓库的 Chart\n  部署\n  安装\n  修改Values的一些属性\n   属性 原始值 新值 说明     mode standalone distributed 集群模式   resources.requests {} requests: cpu: 250m memory: 256Mi    persistence.size 8Gi 1Gi 该值修改为 minio PV 的一半，如果设置成一样大，有可能出现自动创建的 PVC 无法绑定的情况。   persistence.storageClass \u0026ldquo;-\u0026rdquo; local-storage-minio    networkPolicy.enabled false true    networkPolicy.allowExternal true false     具体参数请参考官方文档：https://github.com/bitnami/charts/tree/master/bitnami/minio\n  点击下方的 Submit 按钮提交\n  等待部署完成，即4个Pod都Ready的时候\n  开启minio WebUI\nkubectl port-forward --address 0.0.0.0 svc/minio 9000:9000\r   访问\n账号密码可以通过minio 详情页面查看\n  ","date":1591940892,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1591940892,"objectID":"2817763fe009e0e06c67c6768a266cb1","permalink":"/posts/minio/kubernetes%E5%AE%89%E8%A3%85minio/","publishdate":"2020-06-12T13:48:12+08:00","relpermalink":"/posts/minio/kubernetes%E5%AE%89%E8%A3%85minio/","section":"posts","summary":"通过 helm 在 kubernetes 上安装 minio 前提 3台服务器，CentOS7 系统 已安装K","tags":[],"title":"Kubernetes安装minio","type":"posts"},{"authors":[],"categories":[],"content":"kubernets\n安装 准备工作   开启防火墙端口\n需要开启的端口\nMaster node(s):\rTCP 6443* Kubernetes API Server\rTCP 2379-2380 etcd server client API\rUDP 8472 flannel only\rTCP 10250 Kubelet API\rTCP 10251 kube-scheduler\rTCP 10252 kube-controller-manager\rTCP 10255 Read-Only Kubelet API\rWorker nodes (minions):\rUDP 8472 flannel only\rTCP 10250 Kubelet API\rTCP 10255 Read-Only Kubelet API\rTCP 30000-32767 NodePort Services\r # 主节点\rfirewall-cmd --permanent --add-port=6443/tcp\rfirewall-cmd --permanent --add-port=2379-2380/tcp\rfirewall-cmd --permanent --add-port=10250/tcp\rfirewall-cmd --permanent --add-port=10251/tcp\rfirewall-cmd --permanent --add-port=10252/tcp\rfirewall-cmd --permanent --add-port=10255/tcp\rfirewall-cmd --permanent --add-port=8472/udp\rfirewall-cmd --add-masquerade --permanent\r# only if you want NodePorts exposed on control plane IP as well\rfirewall-cmd --permanent --add-port=30000-32767/tcp\rsystemctl restart firewalld\r# 从节点\rfirewall-cmd --permanent --add-port=10250/tcp\rfirewall-cmd --permanent --add-port=10255/tcp\rfirewall-cmd --permanent --add-port=8472/udp\rfirewall-cmd --permanent --add-port=30000-32767/tcp\rfirewall-cmd --add-masquerade --permanent\rsystemctl restart firewalld\r   设置hostname\n# 节点1\r# 设置hostname\rsudo hostnamectl set-hostname node1\r# 修改/etc/hosts, 添加以下信息\r192.168.201.120 node1\r192.168.201.121 node2\r192.168.201.122 node3\r# 节点2\r# 设置hostname\rsudo hostnamectl set-hostname node2\r# 修改/etc/hosts, 添加以下信息\r192.168.201.120 node1\r192.168.201.121 node2\r192.168.201.122 node3\r# 节点3\r# 设置hostname\rsudo hostnamectl set-hostname node3\r# 修改/etc/hosts, 添加以下信息\r192.168.201.120 node1\r192.168.201.121 node2\r192.168.201.122 node3\r   安装docker\n# 安装docker仓库\ryum install -y yum-utils\ryum-config-manager \\\r--add-repo \\\rhttps://download.docker.com/linux/centos/docker-ce.repo\r# 安装docker 19.03 版本\ryum install -y docker-ce-19.03.11 docker-ce-cli-19.03.11 containerd.io\r# 安装最新版本\r#yum install -y docker-ce docker-ce-cli containerd.io\r# 启动docker\rsystemctl enable docker \u0026amp;\u0026amp; systemctl start docker\r   设置必需的sysctl参数\nsysctl net.bridge.bridge-nf-call-iptables=1\r   禁用swap\nsudo swapoff -a\rsudo sed -i '/ swap / s/^\\(.*\\)$/#\\1/g' /etc/fstab\r   Kubernetes 程序安装 在线安装方式 kubelet/kubeadm/kubectl yum install -y kubelet kubeadm kubectl --disableexcludes=kubernetes\r# 开启kubelet\rsystemctl enable kubelet \u0026amp;\u0026amp; systemctl start kubelet\r 镜像 # 预先拉去镜像\rkubeadm config images pull\r 离线安装方式 下载程序包(翻墙) # 下载程序\ryum install -y --downloadonly kubelet kubeadm kubectl --disableexcludes=kubernetes\r# 下载镜像\rkubeadm config images pull\rdocker pull quay.io/coreos/flannel:v0.12.0-amd64\r# 将镜像保存为tar文件\rdocker save ...\r kubelet/kubeadm/kubectl # 将程序包拷贝到服务器上\r# 进入rpm文件路径\ryum -y install *.rpm\r# 开启kubelet\rsystemctl enable kubelet \u0026amp;\u0026amp; systemctl start kubelet\r 镜像 # 将kubernetes相关镜像拷贝到服务器上\r# 进入镜像文件目录, 加载所有镜像\rfor i in `ls`; do docker load \u0026lt; $i; done\r 离线文件下载地址(1.18版本)\n部署 主节点   初始化\n# flannel\rkubeadm init --pod-network-cidr=10.244.0.0/16\r   配置环境变量\n  root\n# /etc/profile 添加以下内容\rexport KUBECONFIG=/etc/kubernetes/admin.conf\r# 立即生效\rsource /etc/profile\r   常规用户\nmkdir -p $HOME/.kube\rsudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config\rsudo chown $(id -u):$(id -g) $HOME/.kube/config\r     设置网络\nkubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml\r   从节点   加入集群\n# 此命令从主节点init命令输出获得\rkubeadm join 192.168.201.120:6443 --token z8clj6.i8u6s0deqsncftaz --discovery-token-ca-cert-hash sha256:a0cf54fda227bb64d6af37479ed055aad31e55df287566d3793825705355f3d8\r   检查状态(Ready) # 主节点执行\rkubectl get nodes\r# 输出如下\rNAME STATUS ROLES AGE VERSION\rnode1 Ready master 21m v1.18.3\rnode2 NotReady \u0026lt;none\u0026gt; 37s v1.18.3\rnode3 NotReady \u0026lt;none\u0026gt; 36s v1.18.3\r# 等待一会, 全部Ready\rNAME STATUS ROLES AGE VERSION\rnode1 Ready master 22m v1.18.3\rnode2 Ready \u0026lt;none\u0026gt; 2m25s v1.18.3\rnode3 Ready \u0026lt;none\u0026gt; 2m24s v1.18.3\r Dashboard 外部访问   Dashboard 服务类型 由 ClusterIP 改为 NodePort\nkubectl -n kube-system edit service kubernetes-dashboard\r   创建 Admin Service Account\napiVersion: v1\rkind: ServiceAccount\rmetadata:\rname: admin-user\rnamespace: kube-system\r $ kubectl apply -f dashboard-adminuser.yml\rserviceaccount/admin-user created\r   创建 ClusterRoleBinding\napiVersion: rbac.authorization.k8s.io/v1\rkind: ClusterRoleBinding\rmetadata:\rname: admin-user\rroleRef:\rapiGroup: rbac.authorization.k8s.io\rkind: ClusterRole\rname: cluster-admin\rsubjects:\r- kind: ServiceAccount\rname: admin-user\rnamespace: kube-system\r $ kubectl apply -f admin-role-binding.yml\rclusterrolebinding.rbac.authorization.k8s.io/admin-user created\r   获取 Token\nkubectl -n kube-system describe secret $(kubectl -n kube-system get secret | grep admin-user | awk '{print $1}')\r   登录\n  维护   删除Evicted状态Pod\nkubectl get pods | grep Evicted | awk '{print $1}' | xargs kubectl delete pod\r# 指定命名空间\rkubectl get pods -n ns | grep Evicted | awk '{print $1}' | xargs kubectl delete pod -n ns\r    问题 kubernetes 没有启动，手动启动api-server docker容器后出现错误消息：Error response from daemon: cannot join network of a non running container: 56de51eee0b8e0a327c927889e198b84f65cccd8645a092823350c7c10e530af ## 原因1\r服务器时间不对，重新同步下时间（data -s \u0026quot;\u0026quot;）\r [plugin flannel does not support config version \u0026quot;\u0026rdquo; ## /etc/cni/net.d/10-flannel.conflist, 添加如下内容\r{\r\u0026quot;cniVersion\u0026quot;: \u0026quot;0.3.1\u0026quot;,\r\u0026quot;name\u0026quot;: \u0026quot;cbr0\u0026quot;,\r...\r}\r ","date":1591698960,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1591698960,"objectID":"827e4245c2c4d944a46cb6b810776d7b","permalink":"/posts/kubernetes/kubernetes/","publishdate":"2020-06-09T18:36:00+08:00","relpermalink":"/posts/kubernetes/kubernetes/","section":"posts","summary":"kubernets 安装 准备工作 开启防火墙端口 需要开启的端口 Master node(s): TCP 6443* Kubernetes API Server TCP 2379-2380","tags":[],"title":"Kubernetes","type":"posts"},{"authors":[],"categories":[],"content":"Linux 安装minikube\n1 安装docker $ curl -fsSL https://get.docker.com/ | sh\r$ sudo systemctl start docker\r$ sudo systemctl enable docker\r 2 创建用户 minikube 需要非root用户运行\nadduser developer\rpasswd developer\rusermod -aG sudo developer\r# 如果上面命令出现 usermod: group 'sudo' does not exist, 运行以下命令\rusermod -aG wheel developer\rsu - developer\rsudo usermod -aG docker $USER\r- 重新登录\r 3 安装kubectl $ curl -LO https://storage.googleapis.com/kubernetes-release/release/`curl -s https://storage.googleapis.com/kubernetes-release/release/stable.txt`/bin/linux/amd64/kubectl\r$ chmod +x ./kubectl\r$ sudo mv ./kubectl /usr/local/bin/kubectl\r$ kubectl version --client\r 4 安装minikube $ curl -Lo minikube https://storage.googleapis.com/minikube/releases/latest/minikube-linux-amd64 \\\r\u0026amp;\u0026amp; chmod +x minikube\r$ sudo mkdir -p /usr/local/bin/\r$ sudo install minikube /usr/local/bin/\r 5 启动minikube # 需要翻墙\rminikube start --driver=docker\r 6 验证安装 minikube status\r 显示如下\nminikube\rtype: Control Plane\rhost: Running\rkubelet: Running\rapiserver: Running\rkubeconfig: Configured\r ","date":1591609965,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1591609965,"objectID":"e61c4cec64d2d216b7ce452dd3a9ed85","permalink":"/posts/minikube/","publishdate":"2020-06-08T17:52:45+08:00","relpermalink":"/posts/minikube/","section":"posts","summary":"Linux 安装minikube 1 安装docker $ curl -fsSL https://get.docker.com/ | sh $ sudo systemctl start","tags":[],"title":"Minikube安装","type":"posts"},{"authors":[],"categories":[],"content":"goofys挂载minio为本地路径\n1 安装fuse yum -y install fuse\r 2 安装goofys wget https://github.com/kahing/goofys/releases/latest/download/goofys -O /bin/goofys \u0026amp;\u0026amp; chmod +x /bin/goofys\r 3 配置认证信息 $ cat ~/.aws/credentials\r[default]\raws_access_key_id = AKID1234567890\raws_secret_access_key = MY-SECRET-KEY\r 4 使用 $ $GOPATH/bin/goofys \u0026lt;bucket\u0026gt; \u0026lt;mountpoint\u0026gt;\r$ $GOPATH/bin/goofys \u0026lt;bucket:prefix\u0026gt; \u0026lt;mountpoint\u0026gt;\r# 例\rgoofys --endpoint http://localhost:9000 bucket /mnt\r# 前端运行\rgoofys -f --endpoint http://localhost:9000 bucket /mnt\r 5 开机自动挂载 # 配置 /etc/fstab, 添加如下信息\rgoofys#bucket /mnt/mountpoint fuse _netdev,allow_other,--file-mode=0666,--dir-mode=0777,--endpoint=http://localhost:9000 0 0\r ","date":1591599578,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1591599578,"objectID":"7c2b207936e183a17c4401719dc8ee2f","permalink":"/posts/goofys%E6%8C%82%E8%BD%BDminio%E4%B8%BA%E6%9C%AC%E5%9C%B0%E8%B7%AF%E5%BE%84/","publishdate":"2020-06-08T14:59:38+08:00","relpermalink":"/posts/goofys%E6%8C%82%E8%BD%BDminio%E4%B8%BA%E6%9C%AC%E5%9C%B0%E8%B7%AF%E5%BE%84/","section":"posts","summary":"goofys挂载minio为本地路径 1 安装fuse yum -y install fuse 2","tags":[],"title":"Goofys挂载minio为本地路径","type":"posts"},{"authors":[],"categories":[],"content":"Spring Boot HTTPS   生成证书\nkeytool -genkeypair -alias alias_name -keyalg RSA -keysize 2048 -storetype PKCS12 -keystore alias_name.p12 -ext \u0026quot;SAN:c=DNS:localhost,IP:127.0.0.1\u0026quot; -validity 3650\r   将证书放到resources目录下\n  spring boot 配置\nserver:\rport: 8443\rssl:\renabled: true\rkey-alias: alias_name\rkey-store-password: ******\rkey-store: classpath:alias_name.p12\rkey-store-type: PKCS12\r   客户端配置\n// 自定义 RestTemplate\rRestTemplate serverRestTemplateWith() {\rSSLContext sslContext = getSSLContext();\rSSLConnectionSocketFactory socketFactory = new SSLConnectionSocketFactory(sslContext);\rCloseableHttpClient httpClient = HttpClients.custom().setSSLSocketFactory(socketFactory).build();\rHttpComponentsClientHttpRequestFactory factory = new HttpComponentsClientHttpRequestFactory(httpClient);\rfactory.setConnectTimeout(30000);\rfactory.setReadTimeout(30000);\rreturn new RestTemplate(factory);\r}\r// 构建 SSLContext\rSSLContext getSSLContext() {\rtry {\rAppProperties appProperties = appContext.getAppProperties();\rreturn SSLContexts.custom()\r.loadTrustMaterial(ResourceUtils.getFile(appProperties.getServerKeyStore()), appProperties.getServerKeyStorePassword().toCharArray())\r// 跳过证书检查\r// .loadTrustMaterial(null, (X509Certificate[] chain, String authType) -\u0026gt; true)\r.build();\r} catch (Exception e) {\rthrow new RuntimeException(e);\r}\r}\r   生成证书 keytool -genkeypair -alias bc -keyalg RSA -keysize 2048 -storetype PKCS12 -keystore bc.p12 -ext \u0026quot;SAN:c=DNS:localhost,IP:127.0.0.1\u0026quot; -validity 3650\r 问题  问题描述：availableProcessors is already set to [8], rejecting [8]  解决：https://blog.csdn.net/busishenren/article/details/90478928\n问题  ","date":1590122314,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1590122314,"objectID":"06418f91667ced3bf046fbac6d1b8d1c","permalink":"/posts/springboot/","publishdate":"2020-05-22T12:38:34+08:00","relpermalink":"/posts/springboot/","section":"posts","summary":"Spring Boot HTTPS 生成证书 keytool -genkeypair -alias alias_name -keyalg RSA -keysize 2048 -storetype PKCS12 -keystore alias_name.p12 -ext \u0026quot;SAN:c=DNS:localhost,IP:127.0.0.1\u0026quot; -validity 3650 将证书放到re","tags":[],"title":"SpringBoot整合elasticsearch和redis的问题","type":"posts"},{"authors":[],"categories":[],"content":"本文介绍如何使用 helm 安装 Kafka，使用本地存储的方式。\n1 配置 chart 仓库 helm repo add incubator http://storage.googleapis.com/kubernetes-charts-incubator\r 2 创建本地存储StorageClass apiVersion: storage.k8s.io/v1\rkind: StorageClass\rmetadata:\rname: local-storage\rprovisioner: kubernetes.io/no-provisioner\rvolumeBindingMode: WaitForFirstConsumer\r kubectl apply -f local-storage-class.yaml\r 这里绑定模式选择 WaitForFirstConsumer\n默认情况下，Immediate 模式指示在创建 PersistentVolumeClaim 之后发生卷绑定和动态配置。 对于拓扑受限且不能从集群中的所有节点全局访问的存储后端，persistentvolume 将在不知道 Pod 的调度需求的情况下进行绑定或提供。 这可能会导致不可调度的Pod。\n集群管理员可以通过指定 WaitForFirstConsumer 模式来解决这个问题，该模式将延迟 PersistentVolume 的绑定和配置，直到创建使用 PersistentVolumeClaim 的 Pod。 Persistentvolumes 将根据 Pod 的调度约束所指定的拓扑结构进行选择或配给。 这些因素包括但不限于资源需求、节点选择器、Pod关联性和反关联性、污染和耐受性。\nLocal 通过预先创建的 PersistentVolume 绑定支持 WaitForFirstConsumer。\n参考：\nhttps://kubernetes.io/docs/concepts/storage/storage-classes/#volume-binding-mode\nhttps://kubernetes.io/docs/concepts/storage/volumes/#local\n3 创建Kafka和Zookeeper的PersistentVolume  创建Kafka的PV  apiVersion: v1\rkind: PersistentVolume\rmetadata:\rname: local-storage-kafka-0\rspec:\rcapacity:\rstorage: 5Gi\raccessModes:\r- ReadWriteOnce\rpersistentVolumeReclaimPolicy: Retain\rstorageClassName: local-storage\rlocal:\rpath: /localstorage/kafka/data-0\rnodeAffinity:\rrequired:\rnodeSelectorTerms:\r- matchExpressions:\r- key: kubernetes.io/hostname\roperator: In\rvalues:\r- node3\r---\rapiVersion: v1\rkind: PersistentVolume\rmetadata:\rname: local-storage-kafka-1\rspec:\rcapacity:\rstorage: 5Gi\raccessModes:\r- ReadWriteOnce\rpersistentVolumeReclaimPolicy: Retain\rstorageClassName: local-storage\rlocal:\rpath: /localstorage/kafka/data-1\rnodeAffinity:\rrequired:\rnodeSelectorTerms:\r- matchExpressions:\r- key: kubernetes.io/hostname\roperator: In\rvalues:\r- node3\r---\rapiVersion: v1\rkind: PersistentVolume\rmetadata:\rname: local-storage-kafka-2\rspec:\rcapacity:\rstorage: 5Gi\raccessModes:\r- ReadWriteOnce\rpersistentVolumeReclaimPolicy: Retain\rstorageClassName: local-storage\rlocal:\rpath: /localstorage/kafka/data-2\rnodeAffinity:\rrequired:\rnodeSelectorTerms:\r- matchExpressions:\r- key: kubernetes.io/hostname\roperator: In\rvalues:\r- node3\r mkdir -p /localstorage/kafka/data-0\rmkdir -p /localstorage/kafka/data-1\rmkdir -p /localstorage/kafka/data-2\r  创建Zookeeper的PV  apiVersion: v1\rkind: PersistentVolume\rmetadata:\rname: local-storage-zookeeper-0\rspec:\rcapacity:\rstorage: 5Gi\raccessModes:\r- ReadWriteOnce\rpersistentVolumeReclaimPolicy: Retain\rstorageClassName: local-storage\rlocal:\rpath: /localstorage/zookeeper/data-0\rnodeAffinity:\rrequired:\rnodeSelectorTerms:\r- matchExpressions:\r- key: kubernetes.io/hostname\roperator: In\rvalues:\r- node3\r---\rapiVersion: v1\rkind: PersistentVolume\rmetadata:\rname: local-storage-zookeeper-1\rspec:\rcapacity:\rstorage: 5Gi\raccessModes:\r- ReadWriteOnce\rpersistentVolumeReclaimPolicy: Retain\rstorageClassName: local-storage\rlocal:\rpath: /localstorage/zookeeper/data-1\rnodeAffinity:\rrequired:\rnodeSelectorTerms:\r- matchExpressions:\r- key: kubernetes.io/hostname\roperator: In\rvalues:\r- node3\r---\rapiVersion: v1\rkind: PersistentVolume\rmetadata:\rname: local-storage-zookeeper-2\rspec:\rcapacity:\rstorage: 5Gi\raccessModes:\r- ReadWriteOnce\rpersistentVolumeReclaimPolicy: Retain\rstorageClassName: local-storage\rlocal:\rpath: /localstorage/zookeeper/data-2\rnodeAffinity:\rrequired:\rnodeSelectorTerms:\r- matchExpressions:\r- key: kubernetes.io/hostname\roperator: In\rvalues:\r- node3\r mkdir -p /localstorage/zookeeper/data-0\rmkdir -p /localstorage/zookeeper/data-1\rmkdir -p /localstorage/zookeeper/data-2\r 这里需要在节点上手动创建下存储目录 ，Kubernetes不会自动创建它。\n使用本地卷时需要 PersistentVolume nodeAffinity。 它使 Kubernetes 调度程序能够将使用本地卷的 Pods 正确调度到正确的节点。\n4 部署Kafka   创建value文件kafka-values.yaml\nexternal:\renabled: true\rconfigurationOverrides:\r\u0026quot;advertised.listeners\u0026quot;: |-\rEXTERNAL://192.168.1.1:$((31090 + ${KAFKA_BROKER_ID}))\r\u0026quot;listener.security.protocol.map\u0026quot;: |-\rPLAINTEXT:PLAINTEXT,EXTERNAL:PLAINTEXT\rpersistence:\rstorageClass: local-storage\r   这里开启外部访问，配置IP地址（第5行），集群任意节点均可，同时配置storageClass。其他更多配置看官方文档 https://github.com/helm/charts/tree/master/incubator/kafka\n  安装\nhelm install kafka-dev -f kafka-values.yaml incubator/kafka\r   5 查看Kafka版本 kubectl exec kafka-dev-0 -- ls /usr/share/java/kafka | grep kafka\r 问题   no persistent volumes available for this claim and no storage class is set\n可能原因：\n values.yaml 中的 persistence.size 属性大于 PV 中 storage 大小  在PVC绑定PV时通常根据两个条件来绑定，一个是存储的大小，另一个就是访问模式。\n  zookeeper Cannot open channel to 1 at election\n可能原因：\n 启动zookeeper的时候，上一次启动的数据没有删除（挂载PV目录）     ","date":1589858158,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1589858158,"objectID":"742c6efe4cb1b0be80e485c025fd66e7","permalink":"/posts/helm%E5%AE%89%E8%A3%85kfaka/","publishdate":"2020-05-19T11:15:58+08:00","relpermalink":"/posts/helm%E5%AE%89%E8%A3%85kfaka/","section":"posts","summary":"本文介绍如何使用 helm 安装 Kafka，使用本地存储的方式。 1 配置","tags":[],"title":"Helm安装Kfaka","type":"posts"},{"authors":[],"categories":[],"content":"本文介绍如何使用 helm 安装 MySQL，使用本地存储的方式。\n1 创建StorageClass apiVersion: storage.k8s.io/v1\rkind: StorageClass\rmetadata:\rname: local-storage-mysql\rprovisioner: kubernetes.io/no-provisioner\rvolumeBindingMode: WaitForFirstConsumer\r 这里绑定模式选择 WaitForFirstConsumer\n默认情况下，Immediate 模式指示在创建 PersistentVolumeClaim 之后发生卷绑定和动态配置。 对于拓扑受限且不能从集群中的所有节点全局访问的存储后端，persistentvolume 将在不知道 Pod 的调度需求的情况下进行绑定或提供。 这可能会导致不可调度的Pod。\n集群管理员可以通过指定 WaitForFirstConsumer 模式来解决这个问题，该模式将延迟 PersistentVolume 的绑定和配置，直到创建使用 PersistentVolumeClaim 的 Pod。 Persistentvolumes 将根据 Pod 的调度约束所指定的拓扑结构进行选择或配给。 这些因素包括但不限于资源需求、节点选择器、Pod关联性和反关联性、污染和耐受性。\nLocal 通过预先创建的 PersistentVolume 绑定支持 WaitForFirstConsumer。\n参考：\nhttps://kubernetes.io/docs/concepts/storage/storage-classes/#volume-binding-mode\nhttps://kubernetes.io/docs/concepts/storage/volumes/#local\n2 创建PersistentVolume apiVersion: v1\rkind: PersistentVolume\rmetadata:\rname: local-storage-mysql\rspec:\rcapacity:\rstorage: 10Gi\raccessModes:\r- ReadWriteOnce\rpersistentVolumeReclaimPolicy: Retain\rstorageClassName: local-storage-mysql\rlocal:\rpath: /db/mysql\rnodeAffinity:\rrequired:\rnodeSelectorTerms:\r- matchExpressions:\r- key: kubernetes.io/hostname\roperator: In\rvalues:\r- node3\r 使用本地卷时需要 PersistentVolume nodeAffinity。 它使 Kubernetes 调度程序能够将使用本地卷的 Pods 正确调度到正确的节点。\n3 在指定节点上创建存储目录 这里需要在节点 node3 上手动创建存储目录 /db/mysql，Kubernetes不会自动创建它。\n4 安装MySQL helm install my-mysql --set persistence.storageClass=local-storage-mysql stable/mysql\r 这里设置 storageClassName 属性为上面创建的 StorageClass，其他可设置属性参看官方说明 https://github.com/helm/charts/tree/master/stable/mysql#configuration\n","date":1589535447,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1589535447,"objectID":"3441f2efa7d9293ac3436fc44107d519","permalink":"/posts/helm%E5%AE%89%E8%A3%85mysql/","publishdate":"2020-05-15T17:37:27+08:00","relpermalink":"/posts/helm%E5%AE%89%E8%A3%85mysql/","section":"posts","summary":"本文介绍如何使用 helm 安装 MySQL，使用本地存储的方式。 1 创建","tags":[],"title":"Helm安装MySQL","type":"posts"},{"authors":[],"categories":[],"content":"Docker(CentOS) 安装(开发环境) ## 1 更新Yum\rsudo yum update\r## 2 添加docker仓库并安装docker\rcurl -fsSL https://get.docker.com/ | sh\r## 3 启动docker\rsudo systemctl start docker\rsudo systemctl enable docker\r 参考：\nhttps://geekflare.com/docker-installation-guide/\n卸载 sudo yum remove -y docker-ce docker-ce-cli\r 修改存储位置 ## 1 创建文件 /etc/docker/daemon.json, 内容如下\r{\r\u0026quot;data-root\u0026quot;: \u0026quot;/mnt/newlocation\u0026quot;\r}\r## 2 重启docker\rsudo systemctl restart docker\r 删除冗余数据 docker system prune -a\r 删除所有none镜像 docker rmi $(docker images | awk '/^\u0026lt;none\u0026gt;/ { print $3 }')\r 删除所有容器 docker stop $(docker ps -q) \u0026amp; docker rm $(docker ps -aq)\r Dockerfile 时区 FROM alpine:3.6\rRUN apk add --no-cache tzdata\rENV TZ Asia/Shanghai\r examples Spring Boot FROM openjdk:8-jre-alpine\rRUN apk add --no-cache tzdata\rENV TZ Asia/Shanghai\rWORKDIR /app\rCOPY springboot.jar /app\rENTRYPOINT [\u0026quot;java\u0026quot;, \u0026quot;-jar\u0026quot;, \u0026quot;springboot.jar\u0026quot;]\r Docker Compose MySQL version: '3.3'\rservices:\rdb:\rimage: mysql:5.7\rrestart: always\rports:\r- '3306:3306'\renvironment:\rMYSQL_ROOT_PASSWORD: 1qazXSW@\rvolumes:\r- ./mysql/init.sql:/docker-entrypoint-initdb.d/init.sql\r- db-data:/var/lib/mysql\rvolumes:\rdb-data:\r ","date":1589363763,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1589363763,"objectID":"c41c58c31a2bb5b936f9171f54049d4c","permalink":"/posts/docker/","publishdate":"2020-05-13T17:56:03+08:00","relpermalink":"/posts/docker/","section":"posts","summary":"Docker(CentOS) 安装(开发环境) ## 1 更新Yum sudo yum update ## 2 添加docker仓","tags":[],"title":"Docker","type":"posts"},{"authors":[],"categories":[],"content":"安装 1 在github上创建仓库\u0026lt;USERNAME\u0026gt;.github.io\n2 fork Academic Kickstart 仓库，并clone到本地（替换你的用户名）。\ngit clone https://github.com/\u0026lt;USERNAME\u0026gt;/academic-kickstart.git My_Website\rcd My_Website\rgit submodule update --init --recursive\rgit submodule add -f -b master https://github.com/\u0026lt;USERNAME\u0026gt;/\u0026lt;USERNAME\u0026gt;.github.io.git public\r 3 将所有内容添加到本地 git 存储库，并将其推送到 GitHub 上的远程存储库:\ngit add .\rgit commit -m \u0026quot;Initial commit\u0026quot;\rgit push -u origin master\r 4 通过运行 Hugo 重新生成网站的 HTML 代码，并将 public 子模块上传到 GitHub:\nhugo\rcd public\rgit add .\rgit commit -m \u0026quot;Build website\u0026quot;\rgit push origin master\rcd ..\r 5 通过浏览器访问 https://.github.io\n添加内容 1 创建文件\nhugo new posts/my-first-post.md\r 2 部署脚本 deploy.sh\n#!/bin/sh\r# If a command fails then the deploy stops\rset -e\rprintf \u0026quot;\\033[0;32mDeploying updates to GitHub...\\033[0m\\n\u0026quot;\r# Build the project.\rhugo # if using a theme, replace with `hugo -t \u0026lt;YOURTHEME\u0026gt;`\r# Go To Public folder\rcd public\r# Add changes to git.\rgit add .\r# Commit changes.\rmsg=\u0026quot;rebuilding site $(date)\u0026quot;\rif [ -n \u0026quot;$*\u0026quot; ]; then\rmsg=\u0026quot;$*\u0026quot;\rfi\rgit commit -m \u0026quot;$msg\u0026quot;\r# Push source and build repos.\rgit push origin master\r 3 更新网站\n./deploy.sh \u0026quot;Your optional commit message\u0026quot;\r 参考：\nhttps://georgecushen.com/create-your-website-with-hugo/\nhttps://sourcethemes.com/academic/docs/deployment/\n","date":1589355943,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1589355943,"objectID":"c61fafe07bc81ef3e7b82d29f8b7c656","permalink":"/posts/hugo-github-pages/","publishdate":"2020-05-13T15:45:43+08:00","relpermalink":"/posts/hugo-github-pages/","section":"posts","summary":"安装 1 在github上创建仓库\u0026lt;USERNAME\u0026gt","tags":[],"title":"Hugo Github Pages","type":"posts"},{"authors":null,"categories":null,"content":"使用GitLab管理项目 为什么是 GitLab？ GitLab 不仅可以托管代码，它还支持敏捷项目管理。无论是从非常简单的问题跟踪到看板和 Scrum 风格的敏捷交付。在 GitLab 上，您可以像使用专门的项目管理应用程序一样有效地管理问题、可视化整个项目的工作状态并跟踪工作量和解决优先级。相比于其他过于复杂而无法有效使用的应用程序更适合开发人员。\n实现  每组通过独立项目统一管理issue，在Readme中描述使用方式及定义 通过milestone管理项目版本，对齐目标。节奏很重要 通过label管理优先级(P0|P1|P2)、类型(bug|feature) 通过board查看issue进度状态，配合To Do、Doing、Verify等label，定义issue生命周期 通过模板定义author需要提供的信息  Milestone 通过 milestone 管理项目版本，对齐目标。\n短期里程碑 短期内需要达到的一个目标。\n季度里程碑 每个季度发布软件的多个版本。\n发布版本里程碑 一个可发布的版本，项目所有需求都已开发完成并测试通过，可交付给客户。\nIssue 为问题添加一些标签以快速识别涉及应用程序的哪个组件以及它是什么类型的问题（错误、功能请求等）非常重要。GitLab issue boards具有强大的过滤功能，通过过滤 Label 确定优先级并使团队与业务需求保持一致。\n标签示例    分类 Label 简化版Label 说明     类型（type） 🟥type: bug 🟥bug 一个bug    🟦type: feature 🟦feature 新增功能的需求    🟩type: enhancement 🟩enhancement 增强    🟪type: discussion 🟪discussion 问题讨论会议    🟨type: docs 🟨documentation 文档相关的问题    🟥type: security 🟥security 安全性相关的问题    🟨type: test 🟨test 测试相关的问题   优先级（priority） 🟥priority: critical 🟥P0 放下正在做的任何事情并处理它    🟧priority: high 🟧P1 需要尽快处理    🟨priority: medium 🟨P2 按计划进行    ⬜priority: low ⬜P3 当无事可做时处理   状态(state) 🟩state: approved 🟩approved 问题已接收但未处理    🟥state: blocked 🟥blocked 问题由于其他事情阻塞无法进行    🟨state: in progress 🟨in progress 正在处理中    🟦state: completed 🟦completed 已完成    ⬜state: inactive ⬜inactive 问题已结束   组件（component ） 🟩component: frontend 🟩frontend 前端    🟦component: backend 🟦backend 后端   项目（project） 🟧计划 🟧计划     🟩开发中 🟩开发中     ⬜已完成 ⬜已完成     Issue template Bug.md ## 概述 ## Bug行为 ## 期望行为 ## 重现步骤 ## 附件 \u0026lt;!-- URL/相关信息-id 号/截图 --\u0026gt;  Feature.md ## 背景 \u0026lt;!-- 为何要做，不做会有怎样的问题，做了会有怎样的收益 --\u0026gt; ## 需求说明 ## 方案 \u0026lt;!-- 思路或模型 --\u0026gt; ## 验证 \u0026lt;!-- 如何验证，预期的标准是什么 --\u0026gt;  提交约定 建议应用此提交消息格式：\n\u0026lt;type\u0026gt;(\u0026lt;scope\u0026gt;): \u0026lt;subject\u0026gt;  type可以在哪里：\n feat （新特性） fix （bug修复） docs （文档改动） style （格式化, 缺失分号等; 不包括生产代码变动） refactor （重构代码） test （添加缺失的测试, 重构测试, 不包括生产代码变动） chore （更新grunt任务等; 不包括生产代码变动）  scope 可能：\n api front internal 等等。  并且subject必须使用命令式，现在时。\nfeat(api): add users profile endpoint  完整提交消息示例：\n\u0026lt;type\u0026gt;(\u0026lt;scope\u0026gt;): \u0026lt;subject\u0026gt; \u0026lt;body\u0026gt; \u0026lt;footer\u0026gt;  fix(middleware): ensure Range headers adhere more closely to RFC 2616 Add one new dependency, use `range-parser` (Express dependency) to compute range. It is more well-tested in the wild. Fixes #2310  ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"98692b47e74098d1bafbede64e4ae6d9","permalink":"/posts/issue%E9%A1%B9%E7%9B%AE%E7%AE%A1%E7%90%86/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/posts/issue%E9%A1%B9%E7%9B%AE%E7%AE%A1%E7%90%86/","section":"posts","summary":"使用GitLab管理项目 为什么是 GitLab？ GitLab 不仅可以托管","tags":null,"title":"","type":"posts"}]